{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 사전 훈련된 워드 임베딩(Pre-trained Word Embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "케라스 임베딩 층(Keras Embedding Layer)\n",
    "\n",
    "문장의 긍, 부정을 판단하는 감성 분류 모델 예시\n",
    "긍정적인 문장은 레이블1, 부정인 문장은 레이블0 으로 하면"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1 # 패딩을 고려하여 + 1\n",
    "print('단어 집합', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "정수 인코딩 진행"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
     ]
    }
   ],
   "source": [
    "x_encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print(x_encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "가장 길이가 긴 문장 길이"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in x_encoded)\n",
    "print(max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "최대 길이로 모든 샘플에 대해서 패딩 진행"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  0  0]\n",
      " [ 7  8  0  0]\n",
      " [ 9 10  0  0]\n",
      " [11 12  0  0]\n",
      " [13  0  0  0]\n",
      " [14 15  0  0]]\n",
      "[1 0 0 1 1 0 1]\n",
      "[1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_sequences(x_encoded, maxlen=max_len, padding='post')\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "y_train = np.array(y_train)\n",
    "print(y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " 전형적인 이진 분류 모델 설계.\n",
    " 출력층으로 1개 뉴런 배치, 활성화 함수로 시그모이드 손실함수로 binary_crossentropy 사용.\n",
    " 총 100회 학습\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 0s - loss: 0.7065 - acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.7052 - acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.7040 - acc: 0.1429\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.7027 - acc: 0.1429\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.7015 - acc: 0.1429\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.7002 - acc: 0.1429\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.6990 - acc: 0.1429\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.6977 - acc: 0.2857\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.6965 - acc: 0.4286\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.6953 - acc: 0.4286\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.6941 - acc: 0.4286\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 0.6929 - acc: 0.4286\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 0.6917 - acc: 0.4286\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 0.6905 - acc: 0.5714\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 0.6893 - acc: 0.5714\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 0.6881 - acc: 0.5714\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 0.6869 - acc: 0.5714\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 0.6857 - acc: 0.5714\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 0.6845 - acc: 0.5714\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 0.6833 - acc: 0.5714\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 0.6821 - acc: 0.5714\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 0.6809 - acc: 0.5714\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 0.6797 - acc: 0.5714\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 0.6785 - acc: 0.5714\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 0.6773 - acc: 0.5714\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 0.6760 - acc: 0.5714\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 0.6748 - acc: 0.5714\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 0.6736 - acc: 0.5714\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 0.6723 - acc: 0.5714\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 0.6711 - acc: 0.5714\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 0.6698 - acc: 0.5714\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 0.6686 - acc: 0.7143\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 0.6673 - acc: 0.7143\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 0.6660 - acc: 0.7143\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 0.6647 - acc: 0.7143\n",
      "Epoch 36/100\n",
      "1/1 - 0s - loss: 0.6634 - acc: 0.7143\n",
      "Epoch 37/100\n",
      "1/1 - 0s - loss: 0.6621 - acc: 0.7143\n",
      "Epoch 38/100\n",
      "1/1 - 0s - loss: 0.6608 - acc: 0.7143\n",
      "Epoch 39/100\n",
      "1/1 - 0s - loss: 0.6595 - acc: 0.7143\n",
      "Epoch 40/100\n",
      "1/1 - 0s - loss: 0.6581 - acc: 0.7143\n",
      "Epoch 41/100\n",
      "1/1 - 0s - loss: 0.6568 - acc: 0.7143\n",
      "Epoch 42/100\n",
      "1/1 - 0s - loss: 0.6554 - acc: 0.7143\n",
      "Epoch 43/100\n",
      "1/1 - 0s - loss: 0.6541 - acc: 0.7143\n",
      "Epoch 44/100\n",
      "1/1 - 0s - loss: 0.6527 - acc: 0.7143\n",
      "Epoch 45/100\n",
      "1/1 - 0s - loss: 0.6513 - acc: 0.7143\n",
      "Epoch 46/100\n",
      "1/1 - 0s - loss: 0.6499 - acc: 0.7143\n",
      "Epoch 47/100\n",
      "1/1 - 0s - loss: 0.6485 - acc: 0.7143\n",
      "Epoch 48/100\n",
      "1/1 - 0s - loss: 0.6471 - acc: 0.7143\n",
      "Epoch 49/100\n",
      "1/1 - 0s - loss: 0.6456 - acc: 0.7143\n",
      "Epoch 50/100\n",
      "1/1 - 0s - loss: 0.6442 - acc: 0.7143\n",
      "Epoch 51/100\n",
      "1/1 - 0s - loss: 0.6427 - acc: 0.7143\n",
      "Epoch 52/100\n",
      "1/1 - 0s - loss: 0.6412 - acc: 0.7143\n",
      "Epoch 53/100\n",
      "1/1 - 0s - loss: 0.6398 - acc: 0.7143\n",
      "Epoch 54/100\n",
      "1/1 - 0s - loss: 0.6383 - acc: 0.7143\n",
      "Epoch 55/100\n",
      "1/1 - 0s - loss: 0.6368 - acc: 0.7143\n",
      "Epoch 56/100\n",
      "1/1 - 0s - loss: 0.6352 - acc: 0.7143\n",
      "Epoch 57/100\n",
      "1/1 - 0s - loss: 0.6337 - acc: 0.7143\n",
      "Epoch 58/100\n",
      "1/1 - 0s - loss: 0.6322 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 - 0s - loss: 0.6306 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 - 0s - loss: 0.6291 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 - 0s - loss: 0.6275 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 - 0s - loss: 0.6259 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 - 0s - loss: 0.6243 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 - 0s - loss: 0.6227 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 - 0s - loss: 0.6211 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 - 0s - loss: 0.6194 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 - 0s - loss: 0.6178 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 - 0s - loss: 0.6161 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 - 0s - loss: 0.6145 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 - 0s - loss: 0.6128 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 - 0s - loss: 0.6111 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 - 0s - loss: 0.6094 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 - 0s - loss: 0.6077 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 - 0s - loss: 0.6060 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 - 0s - loss: 0.6042 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 - 0s - loss: 0.6025 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 - 0s - loss: 0.6007 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 - 0s - loss: 0.5990 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 - 0s - loss: 0.5972 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 - 0s - loss: 0.5954 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 - 0s - loss: 0.5936 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 - 0s - loss: 0.5918 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 - 0s - loss: 0.5900 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 - 0s - loss: 0.5881 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 - 0s - loss: 0.5863 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 - 0s - loss: 0.5845 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 - 0s - loss: 0.5826 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 - 0s - loss: 0.5807 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 - 0s - loss: 0.5788 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 - 0s - loss: 0.5770 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 - 0s - loss: 0.5751 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 - 0s - loss: 0.5732 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 - 0s - loss: 0.5712 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 - 0s - loss: 0.5693 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 - 0s - loss: 0.5674 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 - 0s - loss: 0.5655 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 - 0s - loss: 0.5635 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 - 0s - loss: 0.5616 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 - 0s - loss: 0.5596 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 - 0s - loss: 0.5576 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1a65bc58b48>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "embedding_dim=4\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=100, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6384478 ]\n",
      " [0.4593276 ]\n",
      " [0.4685587 ]\n",
      " [0.6036079 ]\n",
      " [0.593442  ]\n",
      " [0.47361404]\n",
      " [0.59137696]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(x_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 사전 훈련된 GloVe 사용하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve, urlopen\n",
    "import gzip\n",
    "import zipfile\n",
    "\n",
    "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
    "zf = zipfile.ZipFile('glove.6B.zip')\n",
    "zf.extractall()\n",
    "zf.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:16, 24813.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000개의 Embedding Vector가 있습니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "embedding_dict = dict()\n",
    "f = open('glove.6B\\glove.6B.100d.txt', encoding='utf-8')\n",
    "\n",
    "for line in tqdm(f):\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "\n",
    "    # 100개의 값을 가지는 어레이로 변환\n",
    "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
    "    embedding_dict[word] = word_vector_arr\n",
    "f.close()\n",
    "\n",
    "print('%s개의 Embedding Vector가 있습니다'% len(embedding_dict))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.049773   0.19903    0.10585    0.1391    -0.32395    0.44053\n",
      "  0.3947    -0.22805   -0.25793    0.49768    0.15384   -0.08831\n",
      "  0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\n",
      "  0.74756    0.98256    0.81872    0.28507    0.16178   -0.48626\n",
      " -0.006265  -0.92469   -0.30625   -0.067318  -0.046762  -0.76291\n",
      " -0.0025264 -0.018795   0.12882   -0.52457    0.3586     0.43119\n",
      " -0.89477   -0.057421  -0.53724    0.25587    0.55195    0.44698\n",
      " -0.24252    0.29946    0.25776   -0.8717     0.68426   -0.05688\n",
      " -0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488\n",
      " -0.32507   -0.90171    0.17672    0.075601   0.54896   -0.21488\n",
      " -0.54018   -0.45882   -0.79536    0.26331    0.18879   -0.16363\n",
      "  0.3975     0.1099     0.1164    -0.083499   0.50159    0.35802\n",
      "  0.25677    0.088546   0.42108    0.28674   -0.71285   -0.82915\n",
      "  0.15297   -0.82712    0.022112   1.067     -0.31776    0.1211\n",
      " -0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694\n",
      " -0.0090944  0.1109     0.62543   -0.23682   -0.44928   -0.3667\n",
      " -0.21616   -0.19187   -0.032502   0.38025  ]\n",
      "벡터의 차원 수 100\n"
     ]
    }
   ],
   "source": [
    "print(embedding_dict['respectable'])\n",
    "print('벡터의 차원 수', len(embedding_dict['respectable']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "벡터의 차원수는 100이므로 풀고자 하는 문제와 단어 집합 크기의 행과 100개의 열을 가지는 행렬을 생성. 이 행렬의 값은 모두 0으로 채움"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 행렬의 크기(shape) : (16, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "print('임베딩 행렬의 크기(shape) :', np.shape(embedding_matrix))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "기존 데이터와 맵핑된 정수값 확인"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('highly', 14), ('respectable', 15)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index.items())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "great 값은 2. 사전 훈련된 GloVe 에서의 great 값 확인"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.013786   0.38216    0.53236    0.15261   -0.29694   -0.20558\n",
      " -0.41846   -0.58437   -0.77355   -0.87866   -0.37858   -0.18516\n",
      " -0.128     -0.20584   -0.22925   -0.42599    0.3725     0.26077\n",
      " -1.0702     0.62916   -0.091469   0.70348   -0.4973    -0.77691\n",
      "  0.66045    0.09465   -0.44893    0.018917   0.33146   -0.35022\n",
      " -0.35789    0.030313   0.22253   -0.23236   -0.19719   -0.0053125\n",
      " -0.25848    0.58081   -0.10705   -0.17845   -0.16206    0.087086\n",
      "  0.63029   -0.76649    0.51619    0.14073    1.019     -0.43136\n",
      "  0.46138   -0.43585   -0.47568    0.19226    0.36065    0.78987\n",
      "  0.088945  -2.7814    -0.15366    0.01015    1.1798     0.15168\n",
      " -0.050112   1.2626    -0.77527    0.36031    0.95761   -0.11385\n",
      "  0.28035   -0.02591    0.31246   -0.15424    0.3778    -0.13599\n",
      "  0.2946    -0.31579    0.42943    0.086969   0.019169  -0.27242\n",
      " -0.31696    0.37327    0.61997    0.13889    0.17188    0.30363\n",
      " -1.2776     0.044423  -0.52736   -0.88536   -0.19428   -0.61947\n",
      " -0.10146   -0.26301   -0.061707   0.36627   -0.95223   -0.39346\n",
      " -0.69183   -1.0426     0.28855    0.63056  ]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_dict['great'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "단어 집합의 모든 단어에 대해서 사전 훈련된 GloVe의 임베딩 벡터들을 맵핑 후, 'great'의 벡터 값이 의도한 인덱스의 위치에 삽입 되었는지 확인"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "for word, index in tokenizer.word_index.items():\n",
    "    vector_value = embedding_dict.get(word)\n",
    "    if vector_value is not None:\n",
    "        embedding_matrix[index] = vector_value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "값 확인"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.013786  ,  0.38216001,  0.53236002,  0.15261   , -0.29694   ,\n       -0.20558   , -0.41846001, -0.58437002, -0.77354997, -0.87866002,\n       -0.37858   , -0.18516   , -0.12800001, -0.20584001, -0.22925   ,\n       -0.42598999,  0.3725    ,  0.26076999, -1.07019997,  0.62915999,\n       -0.091469  ,  0.70348001, -0.4973    , -0.77691001,  0.66044998,\n        0.09465   , -0.44893   ,  0.018917  ,  0.33146   , -0.35021999,\n       -0.35789001,  0.030313  ,  0.22253001, -0.23236001, -0.19719   ,\n       -0.0053125 , -0.25848001,  0.58081001, -0.10705   , -0.17845   ,\n       -0.16205999,  0.087086  ,  0.63028997, -0.76648998,  0.51618999,\n        0.14072999,  1.01900005, -0.43136001,  0.46138   , -0.43584999,\n       -0.47567999,  0.19226   ,  0.36065   ,  0.78987002,  0.088945  ,\n       -2.78139997, -0.15366   ,  0.01015   ,  1.17980003,  0.15167999,\n       -0.050112  ,  1.26259995, -0.77526999,  0.36030999,  0.95761001,\n       -0.11385   ,  0.28035   , -0.02591   ,  0.31246001, -0.15424   ,\n        0.37779999, -0.13598999,  0.29460001, -0.31579   ,  0.42943001,\n        0.086969  ,  0.019169  , -0.27241999, -0.31696001,  0.37327   ,\n        0.61997002,  0.13889   ,  0.17188001,  0.30362999, -1.27760005,\n        0.044423  , -0.52736002, -0.88536   , -0.19428   , -0.61947   ,\n       -0.10146   , -0.26301   , -0.061707  ,  0.36627001, -0.95222998,\n       -0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GloVe 에서의 'great 벡터값과 일치함을 보임\n",
    "\n",
    "Embedding layer 에서 embedding_matrix를 초기값으로 설정하면"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 0s - loss: 0.7427 - acc: 0.5357\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.7390 - acc: 0.5000\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.7355 - acc: 0.5000\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.7319 - acc: 0.5000\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.7284 - acc: 0.5000\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.7250 - acc: 0.5357\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.7216 - acc: 0.5357\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.7183 - acc: 0.5357\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.7150 - acc: 0.5357\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.7118 - acc: 0.5357\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.7086 - acc: 0.5357\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 0.7055 - acc: 0.5357\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 0.7024 - acc: 0.5357\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 0.6993 - acc: 0.5357\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 0.6963 - acc: 0.5357\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 0.6933 - acc: 0.5357\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 0.6904 - acc: 0.5357\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 0.6875 - acc: 0.5714\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 0.6846 - acc: 0.5714\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 0.6818 - acc: 0.5714\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 0.6790 - acc: 0.5714\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 0.6762 - acc: 0.5714\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 0.6735 - acc: 0.5714\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 0.6708 - acc: 0.5714\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 0.6682 - acc: 0.5714\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 0.6655 - acc: 0.5714\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 0.6629 - acc: 0.6071\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 0.6604 - acc: 0.6071\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 0.6578 - acc: 0.6071\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 0.6553 - acc: 0.6071\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 0.6528 - acc: 0.6071\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 0.6503 - acc: 0.6429\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 0.6479 - acc: 0.6429\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 0.6455 - acc: 0.6429\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 0.6431 - acc: 0.6429\n",
      "Epoch 36/100\n",
      "1/1 - 0s - loss: 0.6407 - acc: 0.6429\n",
      "Epoch 37/100\n",
      "1/1 - 0s - loss: 0.6384 - acc: 0.6429\n",
      "Epoch 38/100\n",
      "1/1 - 0s - loss: 0.6360 - acc: 0.6429\n",
      "Epoch 39/100\n",
      "1/1 - 0s - loss: 0.6337 - acc: 0.6429\n",
      "Epoch 40/100\n",
      "1/1 - 0s - loss: 0.6314 - acc: 0.6429\n",
      "Epoch 41/100\n",
      "1/1 - 0s - loss: 0.6292 - acc: 0.6429\n",
      "Epoch 42/100\n",
      "1/1 - 0s - loss: 0.6269 - acc: 0.6429\n",
      "Epoch 43/100\n",
      "1/1 - 0s - loss: 0.6247 - acc: 0.6429\n",
      "Epoch 44/100\n",
      "1/1 - 0s - loss: 0.6225 - acc: 0.6429\n",
      "Epoch 45/100\n",
      "1/1 - 0s - loss: 0.6203 - acc: 0.6429\n",
      "Epoch 46/100\n",
      "1/1 - 0s - loss: 0.6181 - acc: 0.6429\n",
      "Epoch 47/100\n",
      "1/1 - 0s - loss: 0.6160 - acc: 0.6429\n",
      "Epoch 48/100\n",
      "1/1 - 0s - loss: 0.6139 - acc: 0.6429\n",
      "Epoch 49/100\n",
      "1/1 - 0s - loss: 0.6117 - acc: 0.6429\n",
      "Epoch 50/100\n",
      "1/1 - 0s - loss: 0.6097 - acc: 0.6429\n",
      "Epoch 51/100\n",
      "1/1 - 0s - loss: 0.6076 - acc: 0.6429\n",
      "Epoch 52/100\n",
      "1/1 - 0s - loss: 0.6055 - acc: 0.6429\n",
      "Epoch 53/100\n",
      "1/1 - 0s - loss: 0.6035 - acc: 0.6429\n",
      "Epoch 54/100\n",
      "1/1 - 0s - loss: 0.6014 - acc: 0.6429\n",
      "Epoch 55/100\n",
      "1/1 - 0s - loss: 0.5994 - acc: 0.6429\n",
      "Epoch 56/100\n",
      "1/1 - 0s - loss: 0.5974 - acc: 0.6429\n",
      "Epoch 57/100\n",
      "1/1 - 0s - loss: 0.5954 - acc: 0.6429\n",
      "Epoch 58/100\n",
      "1/1 - 0s - loss: 0.5935 - acc: 0.6429\n",
      "Epoch 59/100\n",
      "1/1 - 0s - loss: 0.5915 - acc: 0.6429\n",
      "Epoch 60/100\n",
      "1/1 - 0s - loss: 0.5896 - acc: 0.6429\n",
      "Epoch 61/100\n",
      "1/1 - 0s - loss: 0.5877 - acc: 0.6429\n",
      "Epoch 62/100\n",
      "1/1 - 0s - loss: 0.5858 - acc: 0.6429\n",
      "Epoch 63/100\n",
      "1/1 - 0s - loss: 0.5839 - acc: 0.6429\n",
      "Epoch 64/100\n",
      "1/1 - 0s - loss: 0.5820 - acc: 0.6429\n",
      "Epoch 65/100\n",
      "1/1 - 0s - loss: 0.5801 - acc: 0.6429\n",
      "Epoch 66/100\n",
      "1/1 - 0s - loss: 0.5783 - acc: 0.6429\n",
      "Epoch 67/100\n",
      "1/1 - 0s - loss: 0.5765 - acc: 0.6429\n",
      "Epoch 68/100\n",
      "1/1 - 0s - loss: 0.5747 - acc: 0.6429\n",
      "Epoch 69/100\n",
      "1/1 - 0s - loss: 0.5729 - acc: 0.6429\n",
      "Epoch 70/100\n",
      "1/1 - 0s - loss: 0.5711 - acc: 0.6429\n",
      "Epoch 71/100\n",
      "1/1 - 0s - loss: 0.5693 - acc: 0.6429\n",
      "Epoch 72/100\n",
      "1/1 - 0s - loss: 0.5676 - acc: 0.6429\n",
      "Epoch 73/100\n",
      "1/1 - 0s - loss: 0.5658 - acc: 0.6429\n",
      "Epoch 74/100\n",
      "1/1 - 0s - loss: 0.5641 - acc: 0.6429\n",
      "Epoch 75/100\n",
      "1/1 - 0s - loss: 0.5624 - acc: 0.6429\n",
      "Epoch 76/100\n",
      "1/1 - 0s - loss: 0.5607 - acc: 0.6429\n",
      "Epoch 77/100\n",
      "1/1 - 0s - loss: 0.5590 - acc: 0.6429\n",
      "Epoch 78/100\n",
      "1/1 - 0s - loss: 0.5573 - acc: 0.6429\n",
      "Epoch 79/100\n",
      "1/1 - 0s - loss: 0.5557 - acc: 0.6429\n",
      "Epoch 80/100\n",
      "1/1 - 0s - loss: 0.5540 - acc: 0.6429\n",
      "Epoch 81/100\n",
      "1/1 - 0s - loss: 0.5524 - acc: 0.6429\n",
      "Epoch 82/100\n",
      "1/1 - 0s - loss: 0.5508 - acc: 0.6429\n",
      "Epoch 83/100\n",
      "1/1 - 0s - loss: 0.5492 - acc: 0.6429\n",
      "Epoch 84/100\n",
      "1/1 - 0s - loss: 0.5476 - acc: 0.6429\n",
      "Epoch 85/100\n",
      "1/1 - 0s - loss: 0.5460 - acc: 0.6429\n",
      "Epoch 86/100\n",
      "1/1 - 0s - loss: 0.5445 - acc: 0.6429\n",
      "Epoch 87/100\n",
      "1/1 - 0s - loss: 0.5429 - acc: 0.6429\n",
      "Epoch 88/100\n",
      "1/1 - 0s - loss: 0.5414 - acc: 0.6429\n",
      "Epoch 89/100\n",
      "1/1 - 0s - loss: 0.5399 - acc: 0.6429\n",
      "Epoch 90/100\n",
      "1/1 - 0s - loss: 0.5383 - acc: 0.6429\n",
      "Epoch 91/100\n",
      "1/1 - 0s - loss: 0.5368 - acc: 0.6429\n",
      "Epoch 92/100\n",
      "1/1 - 0s - loss: 0.5354 - acc: 0.6429\n",
      "Epoch 93/100\n",
      "1/1 - 0s - loss: 0.5339 - acc: 0.6786\n",
      "Epoch 94/100\n",
      "1/1 - 0s - loss: 0.5324 - acc: 0.6786\n",
      "Epoch 95/100\n",
      "1/1 - 0s - loss: 0.5310 - acc: 0.6786\n",
      "Epoch 96/100\n",
      "1/1 - 0s - loss: 0.5295 - acc: 0.6786\n",
      "Epoch 97/100\n",
      "1/1 - 0s - loss: 0.5281 - acc: 0.6786\n",
      "Epoch 98/100\n",
      "1/1 - 0s - loss: 0.5267 - acc: 0.6786\n",
      "Epoch 99/100\n",
      "1/1 - 0s - loss: 0.5253 - acc: 0.6786\n",
      "Epoch 100/100\n",
      "1/1 - 0s - loss: 0.5239 - acc: 0.7143\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1a66a23da08>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "output_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, output_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "# 가중치 초기값 embedding_matrix\n",
    "# input_length 는 입력 시퀀스의 길이\n",
    "# trainable 은 사전 훈련된 워드 임베딩을 사용했으므로 추가 훈련을 하지 않는 다는 뜻\n",
    "model.add(e)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=100, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 사전 훈련된 Word2Vec 사용하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\n",
    "                           filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "print('모델의 크기(shape) :',word2vec_model.vectors.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "print('임베딩 행렬의 크기(shape) :', np.shape(embedding_matrix))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "특정 단어의 임베딩 벡터를 리턴 받을 수 있는 get_vector 함수 구현"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    else:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "단어 집합으로부터 단어를 1개씩 호출하여 word2vec_model에 해당 단어의 임베딩 벡터값이 존재하는지 확인.\n",
    "\n",
    "만약 None이 아니라면 존재한다는 의미이므로 임베딩 행렬에 해당 단어의 인덱스 위치의 행에 임베딩 벡터의 값을 저장"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for word, index in tokenizer.word_index.items():\n",
    "    vector_value = get_vector(word)\n",
    "    if  vector_value is not None:\n",
    "        embedding_matrix[index] = vector_value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "모델 설계"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(max_len,), dtype='int32'))\n",
    "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc']\n",
    ")\n",
    "model.fit(x_train, y_train, epochs=100, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}