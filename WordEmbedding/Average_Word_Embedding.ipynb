{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 문서 임베딩 : 워드 임베딩의 평균(Average Word Embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 리뷰 개수 : 25000\n",
      "테스트용 리뷰 개수 : 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwon_notebook\\anaconda3\\envs\\tensorNN\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\kwon_notebook\\anaconda3\\envs\\tensorNN\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 2000\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "print('훈련용 리뷰 개수 :', len(x_train))\n",
    "print('테스트용 리뷰 개수 :', len(y_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n       list([1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 2, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 2, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 2, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 656, 245, 2, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2, 311, 12, 16, 2, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 2, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2, 83, 68, 2, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n       list([1, 4, 2, 2, 33, 2, 4, 2, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 2, 744, 35, 2, 761, 61, 2, 452, 2, 4, 985, 7, 2, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2, 31, 8, 4, 687, 23, 4, 2, 2, 6, 2, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 2, 4, 1615, 5, 2, 7, 2, 17, 13, 2, 12, 19, 6, 464, 31, 314, 11, 2, 6, 719, 605, 11, 8, 202, 27, 310, 4, 2, 2, 8, 2, 58, 10, 10, 537, 2, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2, 182, 5, 17, 75, 2, 922, 36, 279, 131, 2, 17, 2, 42, 17, 35, 921, 2, 192, 5, 1219, 2, 19, 2, 217, 2, 1710, 537, 2, 1236, 5, 736, 10, 10, 61, 403, 9, 2, 40, 61, 2, 5, 27, 2, 159, 90, 263, 2, 2, 309, 8, 178, 5, 82, 2, 4, 65, 15, 2, 145, 143, 2, 12, 2, 537, 746, 537, 537, 15, 2, 4, 2, 594, 7, 2, 94, 2, 2, 2, 11, 2, 4, 538, 7, 1795, 246, 2, 9, 2, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2, 936, 5, 2, 2, 19, 49, 7, 4, 1885, 2, 1118, 25, 80, 126, 842, 10, 10, 2, 2, 2, 27, 2, 11, 1550, 2, 159, 27, 341, 29, 2, 19, 2, 173, 7, 90, 2, 8, 30, 11, 4, 1784, 86, 1117, 8, 2, 46, 11, 2, 21, 29, 9, 2, 23, 4, 1010, 2, 793, 6, 2, 1386, 1830, 10, 10, 246, 50, 9, 6, 2, 1944, 746, 90, 29, 2, 8, 124, 4, 882, 4, 882, 496, 27, 2, 2, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 2, 1311, 8, 4, 2, 7, 31, 7, 2, 91, 2, 2, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 2, 1775, 2, 2, 1846, 4, 2, 7, 154, 5, 4, 518, 53, 2, 2, 7, 2, 882, 11, 399, 38, 75, 257, 2, 19, 2, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 2, 4, 2, 2, 9, 242, 4, 91, 1202, 2, 5, 2, 307, 22, 7, 2, 126, 93, 40, 2, 13, 188, 1076, 2, 19, 4, 2, 7, 2, 537, 23, 53, 537, 21, 82, 40, 2, 13, 2, 14, 280, 13, 219, 4, 2, 431, 758, 859, 4, 953, 1052, 2, 7, 2, 5, 94, 40, 25, 238, 60, 2, 4, 2, 804, 2, 7, 4, 2, 132, 8, 67, 6, 22, 15, 9, 283, 8, 2, 14, 31, 9, 242, 955, 48, 25, 279, 2, 23, 12, 1685, 195, 25, 238, 60, 796, 2, 4, 671, 7, 2, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 2, 15, 566, 30, 579, 21, 64, 2]),\n       list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2, 18, 457, 88, 13, 2, 1400, 45, 2, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2, 5, 13, 16, 131, 2, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 2, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 2, 5, 296, 12, 2, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2, 1272, 7, 2, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 2, 18, 165, 170, 143, 19, 14, 5, 2, 6, 226, 251, 7, 61, 113])],\n      dtype=object)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 0, 0, 1, 0], dtype=int64)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "정수 인코딩이 되어있음을 볼 수 있음"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 첫번째 샘플 : [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "훈련 데이터의 첫번째 샘플의 레이블 : 1\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 첫번째 샘플 :', x_train[0])\n",
    "print('훈련 데이터의 첫번째 샘플의 레이블 :', y_train[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 용 리뷰의 평균 길이:  238\n",
      "테스트 용 리뷰의 평균 길이:  230\n"
     ]
    }
   ],
   "source": [
    "print('훈련 용 리뷰의 평균 길이: ', np.mean(list(map(len, x_train)), dtype=int))\n",
    "print('테스트 용 리뷰의 평균 길이: ', np.mean(list(map(len, x_test)), dtype=int))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "400으로 패딩"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train의 크기(shape) : (25000, 400)\n",
      "x_test 의 크기(shape) : (25000, 400)\n"
     ]
    }
   ],
   "source": [
    "max_len = 400\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train의 크기(shape) :', x_train.shape)\n",
    "print('x_test 의 크기(shape) :', x_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 설계하기\n",
    "임베딩 벡터를 평균으로 사용하는 모델을 설계 해보자\n",
    "GlobalAveragePooling1D()는 입력으로 들어오는 모든 벡터들의 평균을 구하는 역할을 함.\n",
    "Embedding() 다음에 GlobalAveragePooling1D()을 추가하면 해당 문장의 모든 단어 벡터들의 평균 벡터를 구함\n",
    "\n",
    "이진 분류를 수행해야 하므로 그 후에는 시그모이드 함수를 활성화 함수로 사용하는 뉴런 1개를 배치.\n",
    "훈련데이터의 20%를 검증 데이터로 사용하고 총 10 에포크 학습"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "612/625 [============================>.] - ETA: 0s - loss: 0.6409 - acc: 0.7073\n",
      "Epoch 00001: val_acc improved from -inf to 0.78900, saving model to embedding_average_model.h5\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.6392 - acc: 0.7092 - val_loss: 0.5466 - val_acc: 0.7890\n",
      "Epoch 2/10\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.4689 - acc: 0.8297\n",
      "Epoch 00002: val_acc improved from 0.78900 to 0.85080, saving model to embedding_average_model.h5\n",
      "625/625 [==============================] - 3s 6ms/step - loss: 0.4686 - acc: 0.8297 - val_loss: 0.4122 - val_acc: 0.8508\n",
      "Epoch 3/10\n",
      "616/625 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8603\n",
      "Epoch 00003: val_acc improved from 0.85080 to 0.85980, saving model to embedding_average_model.h5\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3716 - acc: 0.8609 - val_loss: 0.3586 - val_acc: 0.8598\n",
      "Epoch 4/10\n",
      "612/625 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.8732\n",
      "Epoch 00004: val_acc improved from 0.85980 to 0.86940, saving model to embedding_average_model.h5\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3283 - acc: 0.8735 - val_loss: 0.3339 - val_acc: 0.8694\n",
      "Epoch 5/10\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.8798\n",
      "Epoch 00005: val_acc improved from 0.86940 to 0.87580, saving model to embedding_average_model.h5\n",
      "625/625 [==============================] - 3s 6ms/step - loss: 0.3041 - acc: 0.8798 - val_loss: 0.3191 - val_acc: 0.8758\n",
      "Epoch 6/10\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.8874\n",
      "Epoch 00006: val_acc improved from 0.87580 to 0.87960, saving model to embedding_average_model.h5\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2875 - acc: 0.8873 - val_loss: 0.3119 - val_acc: 0.8796\n",
      "Epoch 7/10\n",
      "617/625 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.8898\n",
      "Epoch 00007: val_acc did not improve from 0.87960\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2774 - acc: 0.8899 - val_loss: 0.3072 - val_acc: 0.8786\n",
      "Epoch 8/10\n",
      "618/625 [============================>.] - ETA: 0s - loss: 0.2683 - acc: 0.8933\n",
      "Epoch 00008: val_acc did not improve from 0.87960\n",
      "625/625 [==============================] - 3s 6ms/step - loss: 0.2682 - acc: 0.8935 - val_loss: 0.3049 - val_acc: 0.8792\n",
      "Epoch 9/10\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.8962\n",
      "Epoch 00009: val_acc did not improve from 0.87960\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2612 - acc: 0.8960 - val_loss: 0.3052 - val_acc: 0.8796\n",
      "Epoch 10/10\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.2561 - acc: 0.8977\n",
      "Epoch 00010: val_acc did not improve from 0.87960\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2560 - acc: 0.8977 - val_loss: 0.3037 - val_acc: 0.8796\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x19f92702dc8>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "embedding_dim = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('embedding_average_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10, callbacks=[es, mc], validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3081 - acc: 0.8756\n",
      "\n",
      " 테스트 정확도: 0.8756\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('embedding_average_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}